
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
  
    <title>2.3. Clustering &mdash; scikit-learn 0.18.dev0 documentation</title>
  <!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" media="screen" />
  <link rel="stylesheet" href="../_static/css/bootstrap-responsive.css"/>

    
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.18.dev0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/js/copybutton.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="top" title="scikit-learn 0.18.dev0 documentation" href="../index.html" />
    <link rel="up" title="2. Unsupervised learning" href="../unsupervised_learning.html" />
    <link rel="next" title="2.4. Biclustering" href="biclustering.html" />
    <link rel="prev" title="2.2. Manifold learning" href="manifold.html" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="../_static/js/bootstrap.min.js" type="text/javascript"></script>
  <link rel="canonical" href="http://scikit-learn.org/stable/modules/clustering.html" />

  <script type="text/javascript">
    $("div.buttonNext, div.buttonPrevious").hover(
       function () {
           $(this).css('background-color', '#FF9C34');
       },
       function () {
           $(this).css('background-color', '#A7D6E2');
       }
    );
  </script>

  </head>
  <body>

<div class="header-wrapper">
    <div class="header">
        <p class="logo"><a href="../index.html">
            <img src="../_static/scikit-learn-logo-small.png" alt="Logo"/>
        </a>
        </p><div class="navbar">
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="../install.html">Installation</a></li>
                <li class="btn-li"><div class="btn-group">
              <a href="../documentation.html">Documentation</a>
              <a class="btn dropdown-toggle" data-toggle="dropdown">
                 <span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
            <li class="link-title">Scikit-learn 0.17 (development)</li>
            <li><a href="../tutorial/index.html">Tutorials</a></li>
            <li><a href="../user_guide.html">User guide</a></li>
            <li><a href="classes.html">API</a></li>
            <li><a href="../faq.html">FAQ</a></li>
            <li><a href="../developers.html">Contributing</a></li>
            <li class="divider"></li>
                <li><a href="http://scikit-learn.org/stable/documentation.html">Scikit-learn 0.16.1 (stable)</a></li>
                <li><a href="http://scikit-learn.org/0.15/documentation.html">Scikit-learn 0.15</a></li>
				<li><a href="../_downloads/user_guide.pdf">PDF documentation</a></li>
              </ul>
            </div>
        </li>
            <li><a href="../auto_examples/index.html">Examples</a></li>
            </ul>

            <div class="search_form">
                <div id="cse" style="width: 100%;"></div>
            </div>
        </div> <!-- end navbar --></div>
</div>


<!-- Github "fork me" ribbon -->
<a href="https://github.com/scikit-learn/scikit-learn">
  <img class="fork-me"
       style="position: absolute; top: 0; right: 0; border: 0;"
       src="../_static/img/forkme.png"
       alt="Fork me on GitHub" />
</a>

<div class="content-wrapper">
    <div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
        <div class="rel">
    

  <!-- rellinks[1:] is an ugly hack to avoid link to module
  index -->
        <div class="rellink">
        <a href="manifold.html"
        accesskey="P">Previous
        <br/>
        <span class="smallrellink">
        2.2. Manifold le...
        </span>
            <span class="hiddenrellink">
            2.2. Manifold learning
            </span>
        </a>
        </div>

    <!-- Ad a link to the 'up' page -->
        <div class="spacer">
        &nbsp;
        </div>
        <div class="rellink">
        <a href="../unsupervised_learning.html">
        Up
        <br/>
        <span class="smallrellink">
        2. Unsupervised ...
        </span>
            <span class="hiddenrellink">
            2. Unsupervised learning
            </span>
            
        </a>
        </div>
    </div>
    
      <p class="doc-version">This documentation is for scikit-learn <strong>version 0.18.dev0</strong> &mdash; <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
    <p class="citing">If you use the software, please consider <a href="../about.html#citing-scikit-learn">citing scikit-learn</a>.</p>
    <ul>
<li><a class="reference internal" href="#">2.3. Clustering</a><ul>
<li><a class="reference internal" href="#overview-of-clustering-methods">2.3.1. Overview of clustering methods</a></li>
<li><a class="reference internal" href="#k-means">2.3.2. K-means</a><ul>
<li><a class="reference internal" href="#mini-batch-k-means">2.3.2.1. Mini Batch K-Means</a></li>
</ul>
</li>
<li><a class="reference internal" href="#affinity-propagation">2.3.3. Affinity Propagation</a></li>
<li><a class="reference internal" href="#mean-shift">2.3.4. Mean Shift</a></li>
<li><a class="reference internal" href="#spectral-clustering">2.3.5. Spectral clustering</a><ul>
<li><a class="reference internal" href="#different-label-assignment-strategies">2.3.5.1. Different label assignment strategies</a></li>
</ul>
</li>
<li><a class="reference internal" href="#hierarchical-clustering">2.3.6. Hierarchical clustering</a><ul>
<li><a class="reference internal" href="#different-linkage-type-ward-complete-and-average-linkage">2.3.6.1. Different linkage type: Ward, complete and average linkage</a></li>
<li><a class="reference internal" href="#adding-connectivity-constraints">2.3.6.2. Adding connectivity constraints</a></li>
<li><a class="reference internal" href="#varying-the-metric">2.3.6.3. Varying the metric</a></li>
</ul>
</li>
<li><a class="reference internal" href="#dbscan">2.3.7. DBSCAN</a></li>
<li><a class="reference internal" href="#birch">2.3.8. Birch</a></li>
<li><a class="reference internal" href="#clustering-performance-evaluation">2.3.9. Clustering performance evaluation</a><ul>
<li><a class="reference internal" href="#adjusted-rand-index">2.3.9.1. Adjusted Rand index</a><ul>
<li><a class="reference internal" href="#advantages">2.3.9.1.1. Advantages</a></li>
<li><a class="reference internal" href="#drawbacks">2.3.9.1.2. Drawbacks</a></li>
<li><a class="reference internal" href="#mathematical-formulation">2.3.9.1.3. Mathematical formulation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#mutual-information-based-scores">2.3.9.2. Mutual Information based scores</a><ul>
<li><a class="reference internal" href="#id10">2.3.9.2.1. Advantages</a></li>
<li><a class="reference internal" href="#id11">2.3.9.2.2. Drawbacks</a></li>
<li><a class="reference internal" href="#id12">2.3.9.2.3. Mathematical formulation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#homogeneity-completeness-and-v-measure">2.3.9.3. Homogeneity, completeness and V-measure</a><ul>
<li><a class="reference internal" href="#id14">2.3.9.3.1. Advantages</a></li>
<li><a class="reference internal" href="#id15">2.3.9.3.2. Drawbacks</a></li>
<li><a class="reference internal" href="#id16">2.3.9.3.3. Mathematical formulation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#silhouette-coefficient">2.3.9.4. Silhouette Coefficient</a><ul>
<li><a class="reference internal" href="#id18">2.3.9.4.1. Advantages</a></li>
<li><a class="reference internal" href="#id19">2.3.9.4.2. Drawbacks</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

    </div>
</div>

<input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
<label for="nav-trigger"></label>




      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="clustering">
<span id="id1"></span><h1>2.3. Clustering<a class="headerlink" href="#clustering" title="Permalink to this headline">Â¶</a></h1>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Cluster_analysis">Clustering</a> of
unlabeled data can be performed with the module <a class="reference internal" href="classes.html#module-sklearn.cluster" title="sklearn.cluster"><tt class="xref py py-mod docutils literal"><span class="pre">sklearn.cluster</span></tt></a>.</p>
<p>Each clustering algorithm comes in two variants: a class, that implements
the <tt class="docutils literal"><span class="pre">fit</span></tt> method to learn the clusters on train data, and a function,
that, given train data, returns an array of integer labels corresponding
to the different clusters. For the class, the labels over the training
data can be found in the <tt class="docutils literal"><span class="pre">labels_</span></tt> attribute.</p>
<div class="topic">
<p class="topic-title first">Input data</p>
<p>One important thing to note is that the algorithms implemented in
this module take different kinds of matrix as input.  On one hand,
<a class="reference internal" href="generated/sklearn.cluster.MeanShift.html#sklearn.cluster.MeanShift" title="sklearn.cluster.MeanShift"><tt class="xref py py-class docutils literal"><span class="pre">MeanShift</span></tt></a> and <a class="reference internal" href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><tt class="xref py py-class docutils literal"><span class="pre">KMeans</span></tt></a> take data matrices of shape
[n_samples, n_features]. These can be obtained from the classes in
the <a class="reference internal" href="classes.html#module-sklearn.feature_extraction" title="sklearn.feature_extraction"><tt class="xref py py-mod docutils literal"><span class="pre">sklearn.feature_extraction</span></tt></a> module. On the other hand,
<a class="reference internal" href="generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation" title="sklearn.cluster.AffinityPropagation"><tt class="xref py py-class docutils literal"><span class="pre">AffinityPropagation</span></tt></a> and <a class="reference internal" href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><tt class="xref py py-class docutils literal"><span class="pre">SpectralClustering</span></tt></a> take
similarity matrices of shape [n_samples, n_samples].  These can be
obtained from the functions in the <a class="reference internal" href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><tt class="xref py py-mod docutils literal"><span class="pre">sklearn.metrics.pairwise</span></tt></a>
module. In other words, <a class="reference internal" href="generated/sklearn.cluster.MeanShift.html#sklearn.cluster.MeanShift" title="sklearn.cluster.MeanShift"><tt class="xref py py-class docutils literal"><span class="pre">MeanShift</span></tt></a> and <a class="reference internal" href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><tt class="xref py py-class docutils literal"><span class="pre">KMeans</span></tt></a> work
with points in a vector space, whereas <a class="reference internal" href="generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation" title="sklearn.cluster.AffinityPropagation"><tt class="xref py py-class docutils literal"><span class="pre">AffinityPropagation</span></tt></a>
and <a class="reference internal" href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><tt class="xref py py-class docutils literal"><span class="pre">SpectralClustering</span></tt></a> can work with arbitrary objects, as
long as a similarity measure exists for such objects.</p>
</div>
<div class="section" id="overview-of-clustering-methods">
<h2>2.3.1. Overview of clustering methods<a class="headerlink" href="#overview-of-clustering-methods" title="Permalink to this headline">Â¶</a></h2>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/cluster/plot_cluster_comparison.html"><img alt="../_images/plot_cluster_comparison_0011.png" src="../_images/plot_cluster_comparison_0011.png" style="width: 950.0px; height: 475.0px;" /></a>
<p class="caption">A comparison of the clustering algorithms in scikit-learn</p>
</div>
<table border="1" class="docutils">
<colgroup>
<col width="15%" />
<col width="16%" />
<col width="20%" />
<col width="27%" />
<col width="22%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Method name</th>
<th class="head">Parameters</th>
<th class="head">Scalability</th>
<th class="head">Usecase</th>
<th class="head">Geometry (metric used)</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><a class="reference internal" href="#k-means"><em>K-Means</em></a></td>
<td>number of clusters</td>
<td>Very large <tt class="docutils literal"><span class="pre">n_samples</span></tt>, medium <tt class="docutils literal"><span class="pre">n_clusters</span></tt> with
<a class="reference internal" href="#mini-batch-kmeans"><em>MiniBatch code</em></a></td>
<td>General-purpose, even cluster size, flat geometry, not too many clusters</td>
<td>Distances between points</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#affinity-propagation"><em>Affinity propagation</em></a></td>
<td>damping, sample preference</td>
<td>Not scalable with n_samples</td>
<td>Many clusters, uneven cluster size, non-flat geometry</td>
<td>Graph distance (e.g. nearest-neighbor graph)</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mean-shift"><em>Mean-shift</em></a></td>
<td>bandwidth</td>
<td>Not scalable with <tt class="docutils literal"><span class="pre">n_samples</span></tt></td>
<td>Many clusters, uneven cluster size, non-flat geometry</td>
<td>Distances between points</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#spectral-clustering"><em>Spectral clustering</em></a></td>
<td>number of clusters</td>
<td>Medium <tt class="docutils literal"><span class="pre">n_samples</span></tt>, small <tt class="docutils literal"><span class="pre">n_clusters</span></tt></td>
<td>Few clusters, even cluster size, non-flat geometry</td>
<td>Graph distance (e.g. nearest-neighbor graph)</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#hierarchical-clustering"><em>Ward hierarchical clustering</em></a></td>
<td>number of clusters</td>
<td>Large <tt class="docutils literal"><span class="pre">n_samples</span></tt> and <tt class="docutils literal"><span class="pre">n_clusters</span></tt></td>
<td>Many clusters, possibly connectivity constraints</td>
<td>Distances between points</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#hierarchical-clustering"><em>Agglomerative clustering</em></a></td>
<td>number of clusters, linkage type, distance</td>
<td>Large <tt class="docutils literal"><span class="pre">n_samples</span></tt> and <tt class="docutils literal"><span class="pre">n_clusters</span></tt></td>
<td>Many clusters, possibly connectivity constraints, non Euclidean
distances</td>
<td>Any pairwise distance</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#dbscan"><em>DBSCAN</em></a></td>
<td>neighborhood size</td>
<td>Very large <tt class="docutils literal"><span class="pre">n_samples</span></tt>, medium <tt class="docutils literal"><span class="pre">n_clusters</span></tt></td>
<td>Non-flat geometry, uneven cluster sizes</td>
<td>Distances between nearest points</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="mixture.html#mixture"><em>Gaussian mixtures</em></a></td>
<td>many</td>
<td>Not scalable</td>
<td>Flat geometry, good for density estimation</td>
<td>Mahalanobis distances to  centers</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#birch"><em>Birch</em></a></td>
<td>branching factor, threshold, optional global clusterer.</td>
<td>Large <tt class="docutils literal"><span class="pre">n_clusters</span></tt> and <tt class="docutils literal"><span class="pre">n_samples</span></tt></td>
<td>Large dataset, outlier removal, data reduction.</td>
<td>Euclidean distance between points</td>
</tr>
</tbody>
</table>
<p>Non-flat geometry clustering is useful when the clusters have a specific
shape, i.e. a non-flat manifold, and the standard euclidean distance is
not the right metric. This case arises in the two top rows of the figure
above.</p>
<p>Gaussian mixture models, useful for clustering, are described in
<a class="reference internal" href="mixture.html#mixture"><em>another chapter of the documentation</em></a> dedicated to
mixture models. KMeans can be seen as a special case of Gaussian mixture
model with equal covariance per component.</p>
</div>
<div class="section" id="k-means">
<span id="id3"></span><h2>2.3.2. K-means<a class="headerlink" href="#k-means" title="Permalink to this headline">Â¶</a></h2>
<p>The <a class="reference internal" href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><tt class="xref py py-class docutils literal"><span class="pre">KMeans</span></tt></a> algorithm clusters data by trying to separate samples
in n groups of equal variance, minimizing a criterion known as the
<a class="reference external" href="inertia">inertia</a> or within-cluster sum-of-squares.
This algorithm requires the number of clusters to be specified.
It scales well to large number of samples and has been used
across a large range of application areas in many different fields.</p>
<p>The k-means algorithm divides a set of <span class="math">N</span> samples <span class="math">X</span>
into <span class="math">K</span> disjoint clusters <span class="math">C</span>,
each described by the mean <span class="math">\mu_j</span> of the samples in the cluster.
The means are commonly called the cluster &#8220;centroids&#8221;;
note that they are not, in general, points from <span class="math">X</span>,
although they live in the same space.
The K-means algorithm aims to choose centroids
that minimise the <em>inertia</em>, or within-cluster sum of squared criterion:</p>
<div class="math">
<p><span class="math">\sum_{i=0}^{n}\min_{\mu_j \in C}(||x_j - \mu_i||^2)</span></p>
</div><p>Inertia, or the within-cluster sum of squares criterion,
can be recognized as a measure of how internally coherent clusters are.
It suffers from various drawbacks:</p>
<ul class="simple">
<li>Inertia makes the assumption that clusters are convex and isotropic,
which is not always the case. It responds poorly to elongated clusters,
or manifolds with irregular shapes.</li>
<li>Inertia is not a normalized metric: we just know that lower values are
better and zero is optimal. But in very high-dimensional spaces, Euclidean
distances tend to become inflated
(this is an instance of the so-called &#8220;curse of dimensionality&#8221;).
Running a dimensionality reduction algorithm such as <a class="reference external" href="PCA">PCA</a>
prior to k-means clustering can alleviate this problem
and speed up the computations.</li>
</ul>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_kmeans_assumptions.html"><img alt="../_images/plot_kmeans_assumptions_0011.png" class="align-center" src="../_images/plot_kmeans_assumptions_0011.png" style="width: 600.0px; height: 600.0px;" /></a>
<p>K-means is often referred to as Lloyd&#8217;s algorithm. In basic terms, the
algorithm has three steps. The first step chooses the initial centroids, with
the most basic method being to choose <span class="math">k</span> samples from the dataset
<span class="math">X</span>. After initialization, K-means consists of looping between the
two other steps. The first step assigns each sample to its nearest centroid.
The second step creates new centroids by taking the mean value of all of the
samples assigned to each previous centroid. The difference between the old
and the new centroids are computed and the algorithm repeats these last two
steps until this value is less than a threshold. In other words, it repeats
until the centroids do not move significantly.</p>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_kmeans_digits.html"><img alt="../_images/plot_kmeans_digits_0011.png" class="align-right" src="../_images/plot_kmeans_digits_0011.png" style="width: 280.0px; height: 210.0px;" /></a>
<p>K-means is equivalent to the expectation-maximization algorithm
with a small, all-equal, diagonal covariance matrix.</p>
<p>The algorithm can also be understood through the concept of <a class="reference external" href="https://en.wikipedia.org/wiki/Voronoi_diagram">Voronoi diagrams</a>. First the Voronoi diagram of
the points is calculated using the current centroids. Each segment in the
Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated
to the mean of each segment. The algorithm then repeats this until a stopping
criterion is fulfilled. Usually, the algorithm stops when the relative decrease
in the objective function between iterations is less than the given tolerance
value. This is not the case in this implementation: iteration stops when
centroids move less than the tolerance.</p>
<p>Given enough time, K-means will always converge, however this may be to a local
minimum. This is highly dependent on the initialization of the centroids.
As a result, the computation is often done several times, with different
initializations of the centroids. One method to help address this issue is the
k-means++ initialization scheme, which has been implemented in scikit-learn
(use the <tt class="docutils literal"><span class="pre">init='kmeans++'</span></tt> parameter). This initializes the centroids to be
(generally) distant from each other, leading to provably better results than
random initialization, as shown in the reference.</p>
<p>A parameter can be given to allow K-means to be run in parallel, called
<tt class="docutils literal"><span class="pre">n_jobs</span></tt>. Giving this parameter a positive value uses that many processors
(default: 1). A value of -1 uses all available processors, with -2 using one
less, and so on. Parallelization generally speeds up computation at the cost of
memory (in this case, multiple copies of centroids need to be stored, one for
each job).</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">The parallel version of K-Means is broken on OS X when <cite>numpy</cite> uses the
<cite>Accelerate</cite> Framework. This is expected behavior: <cite>Accelerate</cite> can be called
after a fork but you need to execv the subprocess with the Python binary
(which multiprocessing does not do under posix).</p>
</div>
<p>K-means can be used for vector quantization. This is achieved using the
transform method of a trained model of <a class="reference internal" href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><tt class="xref py py-class docutils literal"><span class="pre">KMeans</span></tt></a>.</p>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_kmeans_assumptions.html#example-cluster-plot-kmeans-assumptions-py"><em>Demonstration of k-means assumptions</em></a>: Demonstrating when
k-means performs intuitively and when it does not</li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_kmeans_digits.html#example-cluster-plot-kmeans-digits-py"><em>A demo of K-Means clustering on the handwritten digits data</em></a>: Clustering handwritten digits</li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li><a class="reference external" href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">&#8220;k-means++: The advantages of careful seeding&#8221;</a>
Arthur, David, and Sergei Vassilvitskii,
<em>Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete
algorithms</em>, Society for Industrial and Applied Mathematics (2007)</li>
</ul>
</div>
<div class="section" id="mini-batch-k-means">
<span id="mini-batch-kmeans"></span><h3>2.3.2.1. Mini Batch K-Means<a class="headerlink" href="#mini-batch-k-means" title="Permalink to this headline">Â¶</a></h3>
<p>The <a class="reference internal" href="generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><tt class="xref py py-class docutils literal"><span class="pre">MiniBatchKMeans</span></tt></a> is a variant of the <a class="reference internal" href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><tt class="xref py py-class docutils literal"><span class="pre">KMeans</span></tt></a> algorithm
which uses mini-batches to reduce the computation time, while still attempting
to optimise the same objective function. Mini-batches are subsets of the input
data, randomly sampled in each training iteration. These mini-batches
drastically reduce the amount of computation required to converge to a local
solution. In contrast to other algorithms that reduce the convergence time of
k-means, mini-batch k-means produces results that are generally only slightly
worse than the standard algorithm.</p>
<p>The algorithm iterates between two major steps, similar to vanilla k-means.
In the first step, <span class="math">b</span> samples are drawn randomly from the dataset, to form
a mini-batch. These are then assigned to the nearest centroid. In the second
step, the centroids are updated. In contrast to k-means, this is done on a
per-sample basis. For each sample in the mini-batch, the assigned centroid
is updated by taking the streaming average of the sample and all previous
samples assigned to that centroid. This has the effect of decreasing the
rate of change for a centroid over time. These steps are performed until
convergence or a predetermined number of iterations is reached.</p>
<p><a class="reference internal" href="generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><tt class="xref py py-class docutils literal"><span class="pre">MiniBatchKMeans</span></tt></a> converges faster than <a class="reference internal" href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><tt class="xref py py-class docutils literal"><span class="pre">KMeans</span></tt></a>, but the quality
of the results is reduced. In practice this difference in quality can be quite
small, as shown in the example and cited reference.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/cluster/plot_mini_batch_kmeans.html"><img alt="../_images/plot_mini_batch_kmeans_0011.png" src="../_images/plot_mini_batch_kmeans_0011.png" style="width: 800.0px; height: 300.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_mini_batch_kmeans.html#example-cluster-plot-mini-batch-kmeans-py"><em>Comparison of the K-Means and MiniBatchKMeans clustering algorithms</em></a>: Comparison of KMeans and
MiniBatchKMeans</li>
<li><a class="reference internal" href="../auto_examples/text/document_clustering.html#example-text-document-clustering-py"><em>Clustering text documents using k-means</em></a>: Document clustering using sparse
MiniBatchKMeans</li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_dict_face_patches.html#example-cluster-plot-dict-face-patches-py"><em>Online learning of a dictionary of parts of faces</em></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf">&#8220;Web Scale K-Means clustering&#8221;</a>
D. Sculley, <em>Proceedings of the 19th international conference on World
wide web</em> (2010)</li>
</ul>
</div>
</div>
</div>
<div class="section" id="affinity-propagation">
<span id="id4"></span><h2>2.3.3. Affinity Propagation<a class="headerlink" href="#affinity-propagation" title="Permalink to this headline">Â¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation" title="sklearn.cluster.AffinityPropagation"><tt class="xref py py-class docutils literal"><span class="pre">AffinityPropagation</span></tt></a> creates clusters by sending messages between
pairs of samples until convergence. A dataset is then described using a small
number of exemplars, which are identified as those most representative of other
samples. The messages sent between pairs represent the suitability for one
sample to be the exemplar of the other, which is updated in response to the
values from other pairs. This updating happens iteratively until convergence,
at which point the final exemplars are chosen, and hence the final clustering
is given.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/cluster/plot_affinity_propagation.html"><img alt="../_images/plot_affinity_propagation_0011.png" src="../_images/plot_affinity_propagation_0011.png" style="width: 400.0px; height: 300.0px;" /></a>
</div>
<p>Affinity Propagation can be interesting as it chooses the number of
clusters based on the data provided. For this purpose, the two important
parameters are the <em>preference</em>, which controls how many exemplars are
used, and the <em>damping factor</em>.</p>
<p>The main drawback of Affinity Propagation is its complexity. The
algorithm has a time complexity of the order <span class="math">O(N^2 T)</span>, where <span class="math">N</span>
is the number of samples and <span class="math">T</span> is the number of iterations until
convergence. Further, the memory complexity is of the order
<span class="math">O(N^2)</span> if a dense similarity matrix is used, but reducible if a
sparse similarity matrix is used. This makes Affinity Propagation most
appropriate for small to medium sized datasets.</p>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_affinity_propagation.html#example-cluster-plot-affinity-propagation-py"><em>Demo of affinity propagation clustering algorithm</em></a>: Affinity
Propagation on a synthetic 2D datasets with 3 classes.</li>
<li><a class="reference internal" href="../auto_examples/applications/plot_stock_market.html#example-applications-plot-stock-market-py"><em>Visualizing the stock market structure</em></a> Affinity Propagation on
Financial time series to find groups of companies</li>
</ul>
</div>
<p><strong>Algorithm description:</strong>
The messages sent between points belong to one of two categories. The first is
the responsibility <span class="math">r(i, k)</span>,
which is the accumulated evidence that sample <span class="math">k</span>
should be the exemplar for sample <span class="math">i</span>.
The second is the availability <span class="math">a(i, k)</span>
which is the accumulated evidence that sample <span class="math">i</span>
should choose sample <span class="math">k</span> to be its exemplar,
and considers the values for all other samples that <span class="math">k</span> should
be an exemplar. In this way, exemplars are chosen by samples if they are (1)
similar enough to many samples and (2) chosen by many samples to be
representative of themselves.</p>
<p>More formally, the responsibility of a sample <span class="math">k</span>
to be the exemplar of sample <span class="math">i</span> is given by:</p>
<div class="math">
<p><span class="math">r(i, k) \leftarrow s(i, k) - max [ a(i, \acute{k}) + s(i, \acute{k}) \forall \acute{k} \neq k ]</span></p>
</div><p>Where <span class="math">s(i, k)</span> is the similarity between samples <span class="math">i</span> and <span class="math">k</span>.
The availability of sample <span class="math">k</span>
to be the exemplar of sample <span class="math">i</span> is given by:</p>
<div class="math">
<p><span class="math">a(i, k) \leftarrow min [0, r(k, k) + \sum_{\acute{i}~s.t.~\acute{i} \notin \{i, k\}}{r(\acute{i}, k)}]</span></p>
</div><p>To begin with, all values for <span class="math">r</span> and <span class="math">a</span> are set to zero,
and the calculation of each iterates until convergence.</p>
</div>
<div class="section" id="mean-shift">
<span id="id5"></span><h2>2.3.4. Mean Shift<a class="headerlink" href="#mean-shift" title="Permalink to this headline">Â¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.cluster.MeanShift.html#sklearn.cluster.MeanShift" title="sklearn.cluster.MeanShift"><tt class="xref py py-class docutils literal"><span class="pre">MeanShift</span></tt></a> clustering aims to discover <em>blobs</em> in a smooth density of
samples. It is a centroid based algorithm, which works by updating candidates
for centroids to be the mean of the points within a given region. These
candidates are then filtered in a post-processing stage to eliminate
near-duplicates to form the final set of centroids.</p>
<p>Given a candidate centroid <span class="math">x_i</span> for iteration <span class="math">t</span>, the candidate
is updated according to the following equation:</p>
<div class="math">
<p><span class="math">x_i^{t+1} = x_i^t + m(x_i^t)</span></p>
</div><p>Where <span class="math">N(x_i)</span> is the neighborhood of samples within a given distance
around <span class="math">x_i</span> and <span class="math">m</span> is the <em>mean shift</em> vector that is computed for each
centroid that points towards a region of the maximum increase in the density of points.
This is computed using the following equation, effectively updating a centroid
to be the mean of the samples within its neighborhood:</p>
<div class="math">
<p><span class="math">m(x_i) = \frac{\sum_{x_j \in N(x_i)}K(x_j - x_i)x_j}{\sum_{x_j \in N(x_i)}K(x_j - x_i)}</span></p>
</div><p>The algorithm automatically sets the number of clusters, instead of relying on a
parameter <tt class="docutils literal"><span class="pre">bandwidth</span></tt>, which dictates the size of the region to search through.
This parameter can be set manually, but can be estimated using the provided
<tt class="docutils literal"><span class="pre">estimate_bandwidth</span></tt> function, which is called if the bandwidth is not set.</p>
<p>The algorithm is not highly scalable, as it requires multiple nearest neighbor
searches during the execution of the algorithm. The algorithm is guaranteed to
converge, however the algorithm will stop iterating when the change in centroids
is small.</p>
<p>Labelling a new sample is performed by finding the nearest centroid for a
given sample.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/cluster/plot_mean_shift.html"><img alt="../_images/plot_mean_shift_0011.png" src="../_images/plot_mean_shift_0011.png" style="width: 400.0px; height: 300.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_mean_shift.html#example-cluster-plot-mean-shift-py"><em>A demo of the mean-shift clustering algorithm</em></a>: Mean Shift clustering
on a synthetic 2D datasets with 3 classes.</li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.8968&amp;rep=rep1&amp;type=pdf">&#8220;Mean shift: A robust approach toward feature space analysis.&#8221;</a>
D. Comaniciu and P. Meer, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (2002)</li>
</ul>
</div>
</div>
<div class="section" id="spectral-clustering">
<span id="id6"></span><h2>2.3.5. Spectral clustering<a class="headerlink" href="#spectral-clustering" title="Permalink to this headline">Â¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><tt class="xref py py-class docutils literal"><span class="pre">SpectralClustering</span></tt></a> does a low-dimension embedding of the
affinity matrix between samples, followed by a KMeans in the low
dimensional space. It is especially efficient if the affinity matrix is
sparse and the <a class="reference external" href="http://pyamg.org/">pyamg</a> module is installed.
SpectralClustering requires the number of clusters to be specified. It
works well for a small number of clusters but is not advised when using
many clusters.</p>
<p>For two clusters, it solves a convex relaxation of the <a class="reference external" href="http://www.cs.berkeley.edu/~malik/papers/SM-ncut.pdf">normalised
cuts</a> problem on
the similarity graph: cutting the graph in two so that the weight of the
edges cut is small compared to the weights of the edges inside each
cluster. This criteria is especially interesting when working on images:
graph vertices are pixels, and edges of the similarity graph are a
function of the gradient of the image.</p>
<p class="centered">
<strong><a class="reference external image-reference" href="../auto_examples/cluster/plot_segmentation_toy.html"><img alt="noisy_img" src="../_images/plot_segmentation_toy_0011.png" style="width: 300.0px; height: 300.0px;" /></a>
 <a class="reference external image-reference" href="../auto_examples/cluster/plot_segmentation_toy.html"><img alt="segmented_img" src="../_images/plot_segmentation_toy_0021.png" style="width: 300.0px; height: 300.0px;" /></a>
</strong></p><div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p>Transforming distance to well-behaved similarities</p>
<p>Note that if the values of your similarity matrix are not well
distributed, e.g. with negative values or with a distance matrix
rather than a similarity, the spectral problem will be singular and
the problem not solvable. In which case it is advised to apply a
transformation to the entries of the matrix. For instance, in the
case of a signed distance matrix, is common to apply a heat kernel:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">similarity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">beta</span> <span class="o">*</span> <span class="n">distance</span> <span class="o">/</span> <span class="n">distance</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
</pre></div>
</div>
<p class="last">See the examples for such an application.</p>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_segmentation_toy.html#example-cluster-plot-segmentation-toy-py"><em>Spectral clustering for image segmentation</em></a>: Segmenting objects
from a noisy background using spectral clustering.</li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_lena_segmentation.html#example-cluster-plot-lena-segmentation-py"><em>Segmenting the picture of Lena in regions</em></a>: Spectral clustering
to split the image of lena in regions.</li>
</ul>
</div>
<div class="section" id="different-label-assignment-strategies">
<h3>2.3.5.1. Different label assignment strategies<a class="headerlink" href="#different-label-assignment-strategies" title="Permalink to this headline">Â¶</a></h3>
<p>Different label assignment strategies can be used, corresponding to the
<tt class="docutils literal"><span class="pre">assign_labels</span></tt> parameter of <a class="reference internal" href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><tt class="xref py py-class docutils literal"><span class="pre">SpectralClustering</span></tt></a>.
The <tt class="docutils literal"><span class="pre">&quot;kmeans&quot;</span></tt> strategy can match finer details of the data, but it can be
more unstable. In particular, unless you control the <tt class="docutils literal"><span class="pre">random_state</span></tt>, it
may not be reproducible from run-to-run, as it depends on a random
initialization. On the other hand, the <tt class="docutils literal"><span class="pre">&quot;discretize&quot;</span></tt> strategy is 100%
reproducible, but it tends to create parcels of fairly even and
geometrical shape.</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><tt class="docutils literal"><span class="pre">assign_labels=&quot;kmeans&quot;</span></tt></th>
<th class="head"><tt class="docutils literal"><span class="pre">assign_labels=&quot;discretize&quot;</span></tt></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><a class="reference external" href="../auto_examples/cluster/plot_lena_segmentation.html"><img alt="lena_kmeans" src="../_images/plot_lena_segmentation_0011.png" style="width: 325.0px; height: 325.0px;" /></a></td>
<td><a class="reference external" href="../auto_examples/cluster/plot_lena_segmentation.html"><img alt="lena_discretize" src="../_images/plot_lena_segmentation_0021.png" style="width: 325.0px; height: 325.0px;" /></a></td>
</tr>
</tbody>
</table>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323">&#8220;A Tutorial on Spectral Clustering&#8221;</a>
Ulrike von Luxburg, 2007</li>
<li><a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324">&#8220;Normalized cuts and image segmentation&#8221;</a>
Jianbo Shi, Jitendra Malik, 2000</li>
<li><a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.1501">&#8220;A Random Walks View of Spectral Segmentation&#8221;</a>
Marina Meila, Jianbo Shi, 2001</li>
<li><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.8100">&#8220;On Spectral Clustering: Analysis and an algorithm&#8221;</a>
Andrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001</li>
</ul>
</div>
</div>
</div>
<div class="section" id="hierarchical-clustering">
<span id="id7"></span><h2>2.3.6. Hierarchical clustering<a class="headerlink" href="#hierarchical-clustering" title="Permalink to this headline">Â¶</a></h2>
<p>Hierarchical clustering is a general family of clustering algorithms that
build nested clusters by merging or splitting them successively. This
hierarchy of clusters is represented as a tree (or dendrogram). The root of the
tree is the unique cluster that gathers all the samples, the leaves being the
clusters with only one sample. See the <a class="reference external" href="https://en.wikipedia.org/wiki/Hierarchical_clustering">Wikipedia page</a> for more details.</p>
<p>The <a class="reference internal" href="generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" title="sklearn.cluster.AgglomerativeClustering"><tt class="xref py py-class docutils literal"><span class="pre">AgglomerativeClustering</span></tt></a> object performs a hierarchical clustering
using a bottom up approach: each observation starts in its own cluster, and
clusters are successively merged together. The linkage criteria determines the
metric used for the merge strategy:</p>
<ul class="simple">
<li><strong>Ward</strong> minimizes the sum of squared differences within all clusters. It is a
variance-minimizing approach and in this sense is similar to the k-means
objective function but tackled with an agglomerative hierarchical
approach.</li>
<li><strong>Maximum</strong> or <strong>complete linkage</strong> minimizes the maximum distance between
observations of pairs of clusters.</li>
<li><strong>Average linkage</strong> minimizes the average of the distances between all
observations of pairs of clusters.</li>
</ul>
<p><a class="reference internal" href="generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" title="sklearn.cluster.AgglomerativeClustering"><tt class="xref py py-class docutils literal"><span class="pre">AgglomerativeClustering</span></tt></a> can also scale to large number of samples
when it is used jointly with a connectivity matrix, but is computationally
expensive when no connectivity constraints are added between samples: it
considers at each step all the possible merges.</p>
<div class="topic">
<p class="topic-title first"><a class="reference internal" href="generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration" title="sklearn.cluster.FeatureAgglomeration"><tt class="xref py py-class docutils literal"><span class="pre">FeatureAgglomeration</span></tt></a></p>
<p>The <a class="reference internal" href="generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration" title="sklearn.cluster.FeatureAgglomeration"><tt class="xref py py-class docutils literal"><span class="pre">FeatureAgglomeration</span></tt></a> uses agglomerative clustering to
group together features that look very similar, thus decreasing the
number of features. It is a dimensionality reduction tool, see
<a class="reference internal" href="unsupervised_reduction.html#data-reduction"><em>Unsupervised dimensionality reduction</em></a>.</p>
</div>
<div class="section" id="different-linkage-type-ward-complete-and-average-linkage">
<h3>2.3.6.1. Different linkage type: Ward, complete and average linkage<a class="headerlink" href="#different-linkage-type-ward-complete-and-average-linkage" title="Permalink to this headline">Â¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" title="sklearn.cluster.AgglomerativeClustering"><tt class="xref py py-class docutils literal"><span class="pre">AgglomerativeClustering</span></tt></a> supports Ward, average, and complete
linkage strategies.</p>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_digits_linkage.html"><img alt="../_images/plot_digits_linkage_0011.png" src="../_images/plot_digits_linkage_0011.png" style="width: 258.0px; height: 172.0px;" /></a>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_digits_linkage.html"><img alt="../_images/plot_digits_linkage_0021.png" src="../_images/plot_digits_linkage_0021.png" style="width: 258.0px; height: 172.0px;" /></a>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_digits_linkage.html"><img alt="../_images/plot_digits_linkage_0031.png" src="../_images/plot_digits_linkage_0031.png" style="width: 258.0px; height: 172.0px;" /></a>
<p>Agglomerative cluster has a &#8220;rich get richer&#8221; behavior that leads to
uneven cluster sizes. In this regard, complete linkage is the worst
strategy, and Ward gives the most regular sizes. However, the affinity
(or distance used in clustering) cannot be varied with Ward, thus for non
Euclidean metrics, average linkage is a good alternative.</p>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_digits_linkage.html#example-cluster-plot-digits-linkage-py"><em>Various Agglomerative Clustering on a 2D embedding of digits</em></a>: exploration of the
different linkage strategies in a real dataset.</li>
</ul>
</div>
</div>
<div class="section" id="adding-connectivity-constraints">
<h3>2.3.6.2. Adding connectivity constraints<a class="headerlink" href="#adding-connectivity-constraints" title="Permalink to this headline">Â¶</a></h3>
<p>An interesting aspect of <a class="reference internal" href="generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" title="sklearn.cluster.AgglomerativeClustering"><tt class="xref py py-class docutils literal"><span class="pre">AgglomerativeClustering</span></tt></a> is that
connectivity constraints can be added to this algorithm (only adjacent
clusters can be merged together), through a connectivity matrix that defines
for each sample the neighboring samples following a given structure of the
data. For instance, in the swiss-roll example below, the connectivity
constraints forbid the merging of points that are not adjacent on the swiss
roll, and thus avoid forming clusters that extend across overlapping folds of
the roll.</p>
<p class="centered">
<strong><a class="reference external image-reference" href="../auto_examples/cluster/plot_ward_structured_vs_unstructured.html"><img alt="unstructured" src="../_images/plot_ward_structured_vs_unstructured_0011.png" style="width: 392.0px; height: 294.0px;" /></a>
 <a class="reference external image-reference" href="../auto_examples/cluster/plot_ward_structured_vs_unstructured.html"><img alt="structured" src="../_images/plot_ward_structured_vs_unstructured_0021.png" style="width: 392.0px; height: 294.0px;" /></a>
</strong></p><p>These constraint are useful to impose a certain local structure, but they
also make the algorithm faster, especially when the number of the samples
is high.</p>
<p>The connectivity constraints are imposed via an connectivity matrix: a
scipy sparse matrix that has elements only at the intersection of a row
and a column with indices of the dataset that should be connected. This
matrix can be constructed from a-priori information: for instance, you
may wish to cluster web pages by only merging pages with a link pointing
from one to another. It can also be learned from the data, for instance
using <a class="reference internal" href="generated/sklearn.neighbors.kneighbors_graph.html#sklearn.neighbors.kneighbors_graph" title="sklearn.neighbors.kneighbors_graph"><tt class="xref py py-func docutils literal"><span class="pre">sklearn.neighbors.kneighbors_graph</span></tt></a> to restrict
merging to nearest neighbors as in <a class="reference internal" href="../auto_examples/cluster/plot_agglomerative_clustering.html#example-cluster-plot-agglomerative-clustering-py"><em>this example</em></a>, or
using <a class="reference internal" href="generated/sklearn.feature_extraction.image.grid_to_graph.html#sklearn.feature_extraction.image.grid_to_graph" title="sklearn.feature_extraction.image.grid_to_graph"><tt class="xref py py-func docutils literal"><span class="pre">sklearn.feature_extraction.image.grid_to_graph</span></tt></a> to
enable only merging of neighboring pixels on an image, as in the
<a class="reference internal" href="../auto_examples/cluster/plot_lena_ward_segmentation.html#example-cluster-plot-lena-ward-segmentation-py"><em>Lena</em></a> example.</p>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_lena_ward_segmentation.html#example-cluster-plot-lena-ward-segmentation-py"><em>A demo of structured Ward hierarchical clustering on Lena image</em></a>: Ward clustering
to split the image of lena in regions.</li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_ward_structured_vs_unstructured.html#example-cluster-plot-ward-structured-vs-unstructured-py"><em>Hierarchical clustering: structured vs unstructured ward</em></a>: Example of
Ward algorithm on a swiss-roll, comparison of structured approaches
versus unstructured approaches.</li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html#example-cluster-plot-feature-agglomeration-vs-univariate-selection-py"><em>Feature agglomeration vs. univariate selection</em></a>:
Example of dimensionality reduction with feature agglomeration based on
Ward hierarchical clustering.</li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_agglomerative_clustering.html#example-cluster-plot-agglomerative-clustering-py"><em>Agglomerative clustering with and without structure</em></a></li>
</ul>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p><strong>Connectivity constraints with average and complete linkage</strong></p>
<p class="last">Connectivity constraints and complete or average linkage can enhance
the &#8216;rich getting richer&#8217; aspect of agglomerative clustering,
particularly so if they are built with
<a class="reference internal" href="generated/sklearn.neighbors.kneighbors_graph.html#sklearn.neighbors.kneighbors_graph" title="sklearn.neighbors.kneighbors_graph"><tt class="xref py py-func docutils literal"><span class="pre">sklearn.neighbors.kneighbors_graph</span></tt></a>. In the limit of a small
number of clusters, they tend to give a few macroscopically occupied
clusters and almost empty ones. (see the discussion in
<a class="reference internal" href="../auto_examples/cluster/plot_agglomerative_clustering.html#example-cluster-plot-agglomerative-clustering-py"><em>Agglomerative clustering with and without structure</em></a>).</p>
</div>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_agglomerative_clustering.html"><img alt="../_images/plot_agglomerative_clustering_0011.png" src="../_images/plot_agglomerative_clustering_0011.png" style="width: 380.0px; height: 152.0px;" /></a>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_agglomerative_clustering.html"><img alt="../_images/plot_agglomerative_clustering_0021.png" src="../_images/plot_agglomerative_clustering_0021.png" style="width: 380.0px; height: 152.0px;" /></a>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_agglomerative_clustering.html"><img alt="../_images/plot_agglomerative_clustering_0031.png" src="../_images/plot_agglomerative_clustering_0031.png" style="width: 380.0px; height: 152.0px;" /></a>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_agglomerative_clustering.html"><img alt="../_images/plot_agglomerative_clustering_0041.png" src="../_images/plot_agglomerative_clustering_0041.png" style="width: 380.0px; height: 152.0px;" /></a>
</div>
<div class="section" id="varying-the-metric">
<h3>2.3.6.3. Varying the metric<a class="headerlink" href="#varying-the-metric" title="Permalink to this headline">Â¶</a></h3>
<p>Average and complete linkage can be used with a variety of distances (or
affinities), in particular Euclidean distance (<em>l2</em>), Manhattan distance
(or Cityblock, or <em>l1</em>), cosine distance, or any precomputed affinity
matrix.</p>
<ul class="simple">
<li><em>l1</em> distance is often good for sparse features, or sparse noise: ie
many of the features are zero, as in text mining using occurences of
rare words.</li>
<li><em>cosine</em> distance is interesting because it is invariant to global
scalings of the signal.</li>
</ul>
<p>The guidelines for choosing a metric is to use one that maximizes the
distance between samples in different classes, and minimizes that within
each class.</p>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_agglomerative_clustering_metrics.html"><img alt="../_images/plot_agglomerative_clustering_metrics_0051.png" src="../_images/plot_agglomerative_clustering_metrics_0051.png" style="width: 256.0px; height: 192.0px;" /></a>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_agglomerative_clustering_metrics.html"><img alt="../_images/plot_agglomerative_clustering_metrics_0061.png" src="../_images/plot_agglomerative_clustering_metrics_0061.png" style="width: 256.0px; height: 192.0px;" /></a>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_agglomerative_clustering_metrics.html"><img alt="../_images/plot_agglomerative_clustering_metrics_0071.png" src="../_images/plot_agglomerative_clustering_metrics_0071.png" style="width: 256.0px; height: 192.0px;" /></a>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_agglomerative_clustering_metrics.html#example-cluster-plot-agglomerative-clustering-metrics-py"><em>Agglomerative clustering with different metrics</em></a></li>
</ul>
</div>
</div>
</div>
<div class="section" id="dbscan">
<span id="id8"></span><h2>2.3.7. DBSCAN<a class="headerlink" href="#dbscan" title="Permalink to this headline">Â¶</a></h2>
<p>The <a class="reference internal" href="generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN" title="sklearn.cluster.DBSCAN"><tt class="xref py py-class docutils literal"><span class="pre">DBSCAN</span></tt></a> algorithm views clusters as areas of high density
separated by areas of low density. Due to this rather generic view, clusters
found by DBSCAN can be any shape, as opposed to k-means which assumes that
clusters are convex shaped. The central component to the DBSCAN is the concept
of <em>core samples</em>, which are samples that are in areas of high density. A
cluster is therefore a set of core samples, each close to each other
(measured by some distance measure)
and a set of non-core samples that are close to a core sample (but are not
themselves core samples). There are two parameters to the algorithm,
<tt class="docutils literal"><span class="pre">min_samples</span></tt> and <tt class="docutils literal"><span class="pre">eps</span></tt>,
which define formally what we mean when we say <em>dense</em>.
Higher <tt class="docutils literal"><span class="pre">min_samples</span></tt> or lower <tt class="docutils literal"><span class="pre">eps</span></tt>
indicate higher density necessary to form a cluster.</p>
<p>More formally, we define a core sample as being a sample in the dataset such
that there exist <tt class="docutils literal"><span class="pre">min_samples</span></tt> other samples within a distance of
<tt class="docutils literal"><span class="pre">eps</span></tt>, which are defined as <em>neighbors</em> of the core sample. This tells
us that the core sample is in a dense area of the vector space. A cluster
is a set of core samples, that can be built by recursively by taking a core
sample, finding all of its neighbors that are core samples, finding all of
<em>their</em> neighbors that are core samples, and so on. A cluster also has a
set of non-core samples, which are samples that are neighbors of a core sample
in the cluster but are not themselves core samples. Intuitively, these samples
are on the fringes of a cluster.</p>
<p>Any core sample is part of a cluster, by definition. Further, any cluster has
at least <tt class="docutils literal"><span class="pre">min_samples</span></tt> points in it, following the definition of a core
sample. For any sample that is not a core sample, and does have a
distance higher than <tt class="docutils literal"><span class="pre">eps</span></tt> to any core sample, it is considered an outlier by
the algorithm.</p>
<p>In the figure below, the color indicates cluster membership, with large circles
indicating core samples found by the algorithm. Smaller circles are non-core
samples that are still part of a cluster. Moreover, the outliers are indicated
by black points below.</p>
<p class="centered">
<strong><a class="reference external image-reference" href="../auto_examples/cluster/plot_dbscan.html"><img alt="dbscan_results" src="../_images/plot_dbscan_0011.png" style="width: 400.0px; height: 300.0px;" /></a>
</strong></p><div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_dbscan.html#example-cluster-plot-dbscan-py"><em>Demo of DBSCAN clustering algorithm</em></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">Implementation</p>
<p>The algorithm is non-deterministic, but the core samples will
always belong to the same clusters (although the labels may be
different). The non-determinism comes from deciding to which cluster a
non-core sample belongs. A non-core sample can have a distance lower
than <tt class="docutils literal"><span class="pre">eps</span></tt> to two core samples in different clusters. By the
triangular inequality, those two core samples must be more distant than
<tt class="docutils literal"><span class="pre">eps</span></tt> from each other, or they would be in the same cluster. The non-core
sample is assigned to whichever cluster is generated first, where
the order is determined randomly. Other than the ordering of
the dataset, the algorithm is deterministic, making the results relatively
stable between runs on the same data.</p>
<p>The current implementation uses ball trees and kd-trees
to determine the neighborhood of points,
which avoids calculating the full distance matrix
(as was done in scikit-learn versions before 0.14).
The possibility to use custom metrics is retained;
for details, see <tt class="xref py py-class docutils literal"><span class="pre">NearestNeighbors</span></tt>.</p>
</div>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li>&#8220;A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases
with Noise&#8221;
Ester, M., H. P. Kriegel, J. Sander, and X. Xu,
In Proceedings of the 2nd International Conference on Knowledge Discovery
and Data Mining, Portland, OR, AAAI Press, pp. 226â231. 1996</li>
</ul>
</div>
</div>
<div class="section" id="birch">
<span id="id9"></span><h2>2.3.8. Birch<a class="headerlink" href="#birch" title="Permalink to this headline">Â¶</a></h2>
<p>The <a class="reference internal" href="generated/sklearn.cluster.Birch.html#sklearn.cluster.Birch" title="sklearn.cluster.Birch"><tt class="xref py py-class docutils literal"><span class="pre">Birch</span></tt></a> builds a tree called the Characteristic Feature Tree (CFT)
for the given data. The data is essentially lossy compressed to a set of
Characteristic Feature nodes (CF Nodes). The CF Nodes have a number of
subclusters called Characteristic Feature subclusters (CF Subclusters)
and these CF Subclusters located in the non-terminal CF Nodes
can have CF Nodes as children.</p>
<p>The CF Subclusters hold the necessary information for clustering which prevents
the need to hold the entire input data in memory. This information includes:</p>
<ul class="simple">
<li>Number of samples in a subcluster.</li>
<li>Linear Sum - A n-dimensional vector holding the sum of all samples</li>
<li>Squared Sum - Sum of the squared L2 norm of all samples.</li>
<li>Centroids - To avoid recalculation linear sum / n_samples.</li>
<li>Squared norm of the centroids.</li>
</ul>
<p>The Birch algorithm has two parameters, the threshold and the branching factor.
The branching factor limits the number of subclusters in a node and the
threshold limits the distance between the entering sample and the existing
subclusters.</p>
<p>This algorithm can be viewed as an instance or data reduction method,
since it reduces the input data to a set of subclusters which are obtained directly
from the leaves of the CFT. This reduced data can be further processed by feeding
it into a global clusterer. This global clusterer can be set by <tt class="docutils literal"><span class="pre">n_clusters</span></tt>.
If <tt class="docutils literal"><span class="pre">n_clusters</span></tt> is set to None, the subclusters from the leaves are directly
read off, otherwise a global clustering step labels these subclusters into global
clusters (labels) and the samples are mapped to the global label of the nearest subcluster.</p>
<p><strong>Algorithm description:</strong></p>
<ul class="simple">
<li>A new sample is inserted into the root of the CF Tree which is a CF Node.
It is then merged with the subcluster of the root, that has the smallest
radius after merging, constrained by the threshold and branching factor conditions.
If the subcluster has any child node, then this is done repeatedly till it reaches
a leaf. After finding the nearest subcluster in the leaf, the properties of this
subcluster and the parent subclusters are recursively updated.</li>
<li>If the radius of the subcluster obtained by merging the new sample and the
nearest subcluster is greater than the square of the threshold and if the
number of subclusters is greater than the branching factor, then a space is temporarily
allocated to this new sample. The two farthest subclusters are taken and
the subclusters are divided into two groups on the basis of the distance
between these subclusters.</li>
<li>If this split node has a parent subcluster and there is room
for a new subcluster, then the parent is split into two. If there is no room,
then this node is again split into two and the process is continued
recursively, till it reaches the root.</li>
</ul>
<p><strong>Birch or MiniBatchKMeans?</strong></p>
<blockquote>
<div><ul class="simple">
<li>Birch does not scale very well to high dimensional data. As a rule of thumb if
<tt class="docutils literal"><span class="pre">n_features</span></tt> is greater than twenty, it is generally better to use MiniBatchKMeans.</li>
<li>If the number of instances of data needs to be reduced, or if one wants a
large number of subclusters either as a preprocessing step or otherwise,
Birch is more useful than MiniBatchKMeans.</li>
</ul>
</div></blockquote>
<p><strong>How to use partial_fit?</strong></p>
<p>To avoid the computation of global clustering, for every call of <tt class="docutils literal"><span class="pre">partial_fit</span></tt>
the user is advised</p>
<blockquote>
<div><ol class="arabic simple">
<li>To set <tt class="docutils literal"><span class="pre">n_clusters=None</span></tt> initially</li>
<li>Train all data by multiple calls to partial_fit.</li>
<li>Set <tt class="docutils literal"><span class="pre">n_clusters</span></tt> to a required value using
<tt class="docutils literal"><span class="pre">brc.set_params(n_clusters=n_clusters)</span></tt>.</li>
<li>Call <tt class="docutils literal"><span class="pre">partial_fit</span></tt> finally with no arguments, i.e <tt class="docutils literal"><span class="pre">brc.partial_fit()</span></tt>
which performs the global clustering.</li>
</ol>
</div></blockquote>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_birch_vs_minibatchkmeans.html"><img alt="../_images/plot_birch_vs_minibatchkmeans_0011.png" src="../_images/plot_birch_vs_minibatchkmeans_0011.png" /></a>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li>Tian Zhang, Raghu Ramakrishnan, Maron Livny
BIRCH: An efficient data clustering method for large databases.
<a class="reference external" href="http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf">http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf</a></li>
<li>Roberto Perdisci
JBirch - Java implementation of BIRCH clustering algorithm
<a class="reference external" href="https://code.google.com/p/jbirch/">https://code.google.com/p/jbirch/</a></li>
</ul>
</div>
</div>
<div class="section" id="clustering-performance-evaluation">
<span id="clustering-evaluation"></span><h2>2.3.9. Clustering performance evaluation<a class="headerlink" href="#clustering-performance-evaluation" title="Permalink to this headline">Â¶</a></h2>
<p>Evaluating the performance of a clustering algorithm is not as trivial as
counting the number of errors or the precision and recall of a supervised
classification algorithm. In particular any evaluation metric should not
take the absolute values of the cluster labels into account but rather
if this clustering define separations of the data similar to some ground
truth set of classes or satisfying some assumption such that members
belong to the same class are more similar that members of different
classes according to some similarity metric.</p>
<div class="section" id="adjusted-rand-index">
<span id="adjusted-rand-score"></span><h3>2.3.9.1. Adjusted Rand index<a class="headerlink" href="#adjusted-rand-index" title="Permalink to this headline">Â¶</a></h3>
<p>Given the knowledge of the ground truth class assignments <tt class="docutils literal"><span class="pre">labels_true</span></tt>
and our clustering algorithm assignments of the same samples
<tt class="docutils literal"><span class="pre">labels_pred</span></tt>, the <strong>adjusted Rand index</strong> is a function that measures
the <strong>similarity</strong> of the two assignments, ignoring permutations and <strong>with
chance normalization</strong>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">0.24...</span>
</pre></div>
</div>
<p>One can permute 0 and 1 in the predicted labels, rename 2 to 3, and get
the same score:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">0.24...</span>
</pre></div>
</div>
<p>Furthermore, <a class="reference internal" href="generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score" title="sklearn.metrics.adjusted_rand_score"><tt class="xref py py-func docutils literal"><span class="pre">adjusted_rand_score</span></tt></a> is <strong>symmetric</strong>: swapping the argument
does not change the score. It can thus be used as a <strong>consensus
measure</strong>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">labels_pred</span><span class="p">,</span> <span class="n">labels_true</span><span class="p">)</span>  
<span class="go">0.24...</span>
</pre></div>
</div>
<p>Perfect labeling is scored 1.0:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="n">labels_true</span><span class="p">[:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>
<span class="go">1.0</span>
</pre></div>
</div>
<p>Bad (e.g. independent labelings) have negative or close to 0.0 scores:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">labels_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">-0.12...</span>
</pre></div>
</div>
<div class="section" id="advantages">
<h4>2.3.9.1.1. Advantages<a class="headerlink" href="#advantages" title="Permalink to this headline">Â¶</a></h4>
<ul class="simple">
<li><strong>Random (uniform) label assignments have a ARI score close to 0.0</strong>
for any value of <tt class="docutils literal"><span class="pre">n_clusters</span></tt> and <tt class="docutils literal"><span class="pre">n_samples</span></tt> (which is not the
case for raw Rand index or the V-measure for instance).</li>
<li><strong>Bounded range [-1, 1]</strong>: negative values are bad (independent
labelings), similar clusterings have a positive ARI, 1.0 is the perfect
match score.</li>
<li><strong>No assumption is made on the cluster structure</strong>: can be used
to compare clustering algorithms such as k-means which assumes isotropic
blob shapes with results of spectral clustering algorithms which can
find cluster with &#8220;folded&#8221; shapes.</li>
</ul>
</div>
<div class="section" id="drawbacks">
<h4>2.3.9.1.2. Drawbacks<a class="headerlink" href="#drawbacks" title="Permalink to this headline">Â¶</a></h4>
<ul>
<li><p class="first">Contrary to inertia, <strong>ARI requires knowledge of the ground truth
classes</strong> while is almost never available in practice or requires manual
assignment by human annotators (as in the supervised learning setting).</p>
<p>However ARI can also be useful in a purely unsupervised setting as a
building block for a Consensus Index that can be used for clustering
model selection (TODO).</p>
</li>
</ul>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_adjusted_for_chance_measures.html#example-cluster-plot-adjusted-for-chance-measures-py"><em>Adjustment for chance in clustering performance evaluation</em></a>: Analysis of
the impact of the dataset size on the value of clustering measures
for random assignments.</li>
</ul>
</div>
</div>
<div class="section" id="mathematical-formulation">
<h4>2.3.9.1.3. Mathematical formulation<a class="headerlink" href="#mathematical-formulation" title="Permalink to this headline">Â¶</a></h4>
<p>If C is a ground truth class assignment and K the clustering, let us
define <span class="math">a</span> and <span class="math">b</span> as:</p>
<ul class="simple">
<li><span class="math">a</span>, the number of pairs of elements that are in the same set
in C and in the same set in K</li>
<li><span class="math">b</span>, the number of pairs of elements that are in different sets
in C and in different sets in K</li>
</ul>
<p>The raw (unadjusted) Rand index is then given by:</p>
<div class="math">
<p><span class="math">\text{RI} = \frac{a + b}{C_2^{n_{samples}}}</span></p>
</div><p>Where <span class="math">C_2^{n_{samples}}</span> is the total number of possible pairs
in the dataset (without ordering).</p>
<p>However the RI score does not guarantee that random label assignments
will get a value close to zero (esp. if the number of clusters is in
the same order of magnitude as the number of samples).</p>
<p>To counter this effect we can discount the expected RI <span class="math">E[\text{RI}]</span> of
random labelings by defining the adjusted Rand index as follows:</p>
<div class="math">
<p><span class="math">\text{ARI} = \frac{\text{RI} - E[\text{RI}]}{\max(\text{RI}) - E[\text{RI}]}</span></p>
</div><div class="topic">
<p class="topic-title first">References</p>
<ul class="simple">
<li><a class="reference external" href="http://www.springerlink.com/content/x64124718341j1j0/">Comparing Partitions</a>
L. Hubert and P. Arabie, Journal of Classification 1985</li>
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index">Wikipedia entry for the adjusted Rand index</a></li>
</ul>
</div>
</div>
</div>
<div class="section" id="mutual-information-based-scores">
<span id="mutual-info-score"></span><h3>2.3.9.2. Mutual Information based scores<a class="headerlink" href="#mutual-information-based-scores" title="Permalink to this headline">Â¶</a></h3>
<p>Given the knowledge of the ground truth class assignments <tt class="docutils literal"><span class="pre">labels_true</span></tt> and
our clustering algorithm assignments of the same samples <tt class="docutils literal"><span class="pre">labels_pred</span></tt>, the
<strong>Mutual Information</strong> is a function that measures the <strong>agreement</strong> of the two
assignments, ignoring permutations.  Two different normalized versions of this
measure are available, <strong>Normalized Mutual Information(NMI)</strong> and <strong>Adjusted
Mutual Information(AMI)</strong>. NMI is often used in the literature while AMI was
proposed more recently and is <strong>normalized against chance</strong>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_mutual_info_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">0.22504...</span>
</pre></div>
</div>
<p>One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get
the same score:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_mutual_info_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">0.22504...</span>
</pre></div>
</div>
<p>All, <a class="reference internal" href="generated/sklearn.metrics.mutual_info_score.html#sklearn.metrics.mutual_info_score" title="sklearn.metrics.mutual_info_score"><tt class="xref py py-func docutils literal"><span class="pre">mutual_info_score</span></tt></a>, <a class="reference internal" href="generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score" title="sklearn.metrics.adjusted_mutual_info_score"><tt class="xref py py-func docutils literal"><span class="pre">adjusted_mutual_info_score</span></tt></a> and
<a class="reference internal" href="generated/sklearn.metrics.normalized_mutual_info_score.html#sklearn.metrics.normalized_mutual_info_score" title="sklearn.metrics.normalized_mutual_info_score"><tt class="xref py py-func docutils literal"><span class="pre">normalized_mutual_info_score</span></tt></a> are symmetric: swapping the argument does
not change the score. Thus they can be used as a <strong>consensus measure</strong>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_mutual_info_score</span><span class="p">(</span><span class="n">labels_pred</span><span class="p">,</span> <span class="n">labels_true</span><span class="p">)</span>  
<span class="go">0.22504...</span>
</pre></div>
</div>
<p>Perfect labeling is scored 1.0:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="n">labels_true</span><span class="p">[:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_mutual_info_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>
<span class="go">1.0</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">normalized_mutual_info_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>
<span class="go">1.0</span>
</pre></div>
</div>
<p>This is not true for <tt class="docutils literal"><span class="pre">mutual_info_score</span></tt>, which is therefore harder to judge:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">mutual_info_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">0.69...</span>
</pre></div>
</div>
<p>Bad (e.g. independent labelings) have non-positive scores:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">labels_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_mutual_info_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">-0.10526...</span>
</pre></div>
</div>
<div class="section" id="id10">
<h4>2.3.9.2.1. Advantages<a class="headerlink" href="#id10" title="Permalink to this headline">Â¶</a></h4>
<ul class="simple">
<li><strong>Random (uniform) label assignments have a AMI score close to 0.0</strong>
for any value of <tt class="docutils literal"><span class="pre">n_clusters</span></tt> and <tt class="docutils literal"><span class="pre">n_samples</span></tt> (which is not the
case for raw Mutual Information or the V-measure for instance).</li>
<li><strong>Bounded range [0, 1]</strong>:  Values close to zero indicate two label
assignments that are largely independent, while values close to one
indicate significant agreement. Further, values of exactly 0 indicate
<strong>purely</strong> independent label assignments and a AMI of exactly 1 indicates
that the two label assignments are equal (with or without permutation).</li>
<li><strong>No assumption is made on the cluster structure</strong>: can be used
to compare clustering algorithms such as k-means which assumes isotropic
blob shapes with results of spectral clustering algorithms which can
find cluster with &#8220;folded&#8221; shapes.</li>
</ul>
</div>
<div class="section" id="id11">
<h4>2.3.9.2.2. Drawbacks<a class="headerlink" href="#id11" title="Permalink to this headline">Â¶</a></h4>
<ul>
<li><p class="first">Contrary to inertia, <strong>MI-based measures require the knowledge
of the ground truth classes</strong> while almost never available in practice or
requires manual assignment by human annotators (as in the supervised learning
setting).</p>
<p>However MI-based measures can also be useful in purely unsupervised setting as a
building block for a Consensus Index that can be used for clustering
model selection.</p>
</li>
<li><p class="first">NMI and MI are not adjusted against chance.</p>
</li>
</ul>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_adjusted_for_chance_measures.html#example-cluster-plot-adjusted-for-chance-measures-py"><em>Adjustment for chance in clustering performance evaluation</em></a>: Analysis of
the impact of the dataset size on the value of clustering measures
for random assignments. This example also includes the Adjusted Rand
Index.</li>
</ul>
</div>
</div>
<div class="section" id="id12">
<h4>2.3.9.2.3. Mathematical formulation<a class="headerlink" href="#id12" title="Permalink to this headline">Â¶</a></h4>
<p>Assume two label assignments (of the same N objects), <span class="math">U</span> and <span class="math">V</span>.
Their entropy is the amount of uncertainty for a partition set, defined by:</p>
<div class="math">
<p><span class="math">H(U) = \sum_{i=1}^{|U|}P(i)\log(P(i))</span></p>
</div><p>where <span class="math">P(i) = |U_i| / N</span> is the probability that an object picked at
random from <span class="math">U</span> falls into class <span class="math">U_i</span>. Likewise for <span class="math">V</span>:</p>
<div class="math">
<p><span class="math">H(V) = \sum_{j=1}^{|V|}P'(j)\log(P'(j))</span></p>
</div><p>With <span class="math">P'(j) = |V_j| / N</span>. The mutual information (MI) between <span class="math">U</span>
and <span class="math">V</span> is calculated by:</p>
<div class="math">
<p><span class="math">\text{MI}(U, V) = \sum_{i=1}^{|U|}\sum_{j=1}^{|V|}P(i, j)\log\left(\frac{P(i,j)}{P(i)P'(j)}\right)</span></p>
</div><p>where <span class="math">P(i, j) = |U_i \cap V_j| / N</span> is the probability that an object
picked at random falls into both classes <span class="math">U_i</span> and <span class="math">V_j</span>.</p>
<p>The normalized mutual information is defined as</p>
<div class="math">
<p><span class="math">\text{NMI}(U, V) = \frac{\text{MI}(U, V)}{\sqrt{H(U)H(V)}}</span></p>
</div><p>This value of the mutual information and also the normalized variant is not
adjusted for chance and will tend to increase as the number of different labels
(clusters) increases, regardless of the actual amount of &#8220;mutual information&#8221;
between the label assignments.</p>
<p>The expected value for the mutual information can be calculated using the
following equation, from Vinh, Epps, and Bailey, (2009). In this equation,
<span class="math">a_i = |U_i|</span> (the number of elements in <span class="math">U_i</span>) and
<span class="math">b_j = |V_j|</span> (the number of elements in <span class="math">V_j</span>).</p>
<div class="math">
<p><span class="math">E[\text{MI}(U,V)]=\sum_{i=1}^|U| \sum_{j=1}^|V| \sum_{n_{ij}=(a_i+b_j-N)^+
}^{\min(a_i, b_j)} \frac{n_{ij}}{N}\log \left( \frac{ N.n_{ij}}{a_i b_j}\right)
\frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!
(N-a_i-b_j+n_{ij})!}</span></p>
</div><p>Using the expected value, the adjusted mutual information can then be
calculated using a similar form to that of the adjusted Rand index:</p>
<div class="math">
<p><span class="math">\text{AMI} = \frac{\text{MI} - E[\text{MI}]}{\max(H(U), H(V)) - E[\text{MI}]}</span></p>
</div><div class="topic">
<p class="topic-title first">References</p>
<ul class="simple">
<li>Strehl, Alexander, and Joydeep Ghosh (2002). &#8220;Cluster ensembles â a
knowledge reuse framework for combining multiple partitions&#8221;. Journal of
Machine Learning Research 3: 583â617.
<a class="reference external" href="http://strehl.com/download/strehl-jmlr02.pdf">doi:10.1162/153244303321897735</a>.</li>
<li>Vinh, Epps, and Bailey, (2009). &#8220;Information theoretic measures
for clusterings comparison&#8221;. Proceedings of the 26th Annual International
Conference on Machine Learning - ICML &#8216;09.
<a class="reference external" href="http://dx.doi.org/10.1145/1553374.1553511">doi:10.1145/1553374.1553511</a>.
ISBN 9781605585161.</li>
<li>Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for
Clusterings Comparison: Variants, Properties, Normalization and
Correction for Chance, JMLR
<a class="reference external" href="http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf">http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf</a></li>
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Mutual_Information">Wikipedia entry for the (normalized) Mutual Information</a></li>
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Adjusted_Mutual_Information">Wikipedia entry for the Adjusted Mutual Information</a></li>
</ul>
</div>
</div>
</div>
<div class="section" id="homogeneity-completeness-and-v-measure">
<span id="homogeneity-completeness"></span><h3>2.3.9.3. Homogeneity, completeness and V-measure<a class="headerlink" href="#homogeneity-completeness-and-v-measure" title="Permalink to this headline">Â¶</a></h3>
<p>Given the knowledge of the ground truth class assignments of the samples,
it is possible to define some intuitive metric using conditional entropy
analysis.</p>
<p>In particular Rosenberg and Hirschberg (2007) define the following two
desirable objectives for any cluster assignment:</p>
<ul class="simple">
<li><strong>homogeneity</strong>: each cluster contains only members of a single class.</li>
<li><strong>completeness</strong>: all members of a given class are assigned to the same
cluster.</li>
</ul>
<p>We can turn those concept as scores <a class="reference internal" href="generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score" title="sklearn.metrics.homogeneity_score"><tt class="xref py py-func docutils literal"><span class="pre">homogeneity_score</span></tt></a> and
<a class="reference internal" href="generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score" title="sklearn.metrics.completeness_score"><tt class="xref py py-func docutils literal"><span class="pre">completeness_score</span></tt></a>. Both are bounded below by 0.0 and above by
1.0 (higher is better):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">homogeneity_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">0.66...</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">completeness_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span> 
<span class="go">0.42...</span>
</pre></div>
</div>
<p>Their harmonic mean called <strong>V-measure</strong> is computed by
<a class="reference internal" href="generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score" title="sklearn.metrics.v_measure_score"><tt class="xref py py-func docutils literal"><span class="pre">v_measure_score</span></tt></a>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">v_measure_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>    
<span class="go">0.51...</span>
</pre></div>
</div>
<p>The V-measure is actually equivalent to the mutual information (NMI)
discussed above normalized by the sum of the label entropies <a class="reference internal" href="#b2011" id="id13">[B2011]</a>.</p>
<p>Homogeneity, completeness and V-measure can be computed at once using
<a class="reference internal" href="generated/sklearn.metrics.homogeneity_completeness_v_measure.html#sklearn.metrics.homogeneity_completeness_v_measure" title="sklearn.metrics.homogeneity_completeness_v_measure"><tt class="xref py py-func docutils literal"><span class="pre">homogeneity_completeness_v_measure</span></tt></a> as follows:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">homogeneity_completeness_v_measure</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>
<span class="gp">... </span>                                                     
<span class="go">(0.66..., 0.42..., 0.51...)</span>
</pre></div>
</div>
<p>The following clustering assignment is slightly better, since it is
homogeneous but not complete:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">homogeneity_completeness_v_measure</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>
<span class="gp">... </span>                                                     
<span class="go">(1.0, 0.68..., 0.81...)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><a class="reference internal" href="generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score" title="sklearn.metrics.v_measure_score"><tt class="xref py py-func docutils literal"><span class="pre">v_measure_score</span></tt></a> is <strong>symmetric</strong>: it can be used to evaluate
the <strong>agreement</strong> of two independent assignments on the same dataset.</p>
<p>This is not the case for <a class="reference internal" href="generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score" title="sklearn.metrics.completeness_score"><tt class="xref py py-func docutils literal"><span class="pre">completeness_score</span></tt></a> and
<a class="reference internal" href="generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score" title="sklearn.metrics.homogeneity_score"><tt class="xref py py-func docutils literal"><span class="pre">homogeneity_score</span></tt></a>: both are bound by the relationship:</p>
<div class="last highlight-python"><div class="highlight"><pre><span class="n">homogeneity_score</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">==</span> <span class="n">completeness_score</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="id14">
<h4>2.3.9.3.1. Advantages<a class="headerlink" href="#id14" title="Permalink to this headline">Â¶</a></h4>
<ul class="simple">
<li><strong>Bounded scores</strong>: 0.0 is as bad as it can be, 1.0 is a perfect score.</li>
<li>Intuitive interpretation: clustering with bad V-measure can be
<strong>qualitatively analyzed in terms of homogeneity and completeness</strong>
to better feel what &#8216;kind&#8217; of mistakes is done by the assignment.</li>
<li><strong>No assumption is made on the cluster structure</strong>: can be used
to compare clustering algorithms such as k-means which assumes isotropic
blob shapes with results of spectral clustering algorithms which can
find cluster with &#8220;folded&#8221; shapes.</li>
</ul>
</div>
<div class="section" id="id15">
<h4>2.3.9.3.2. Drawbacks<a class="headerlink" href="#id15" title="Permalink to this headline">Â¶</a></h4>
<ul>
<li><p class="first">The previously introduced metrics are <strong>not normalized with regards to
random labeling</strong>: this means that depending on the number of samples,
clusters and ground truth classes, a completely random labeling will
not always yield the same values for homogeneity, completeness and
hence v-measure. In particular <strong>random labeling won&#8217;t yield zero
scores especially when the number of clusters is large</strong>.</p>
<p>This problem can safely be ignored when the number of samples is more
than a thousand and the number of clusters is less than 10. <strong>For
smaller sample sizes or larger number of clusters it is safer to use
an adjusted index such as the Adjusted Rand Index (ARI)</strong>.</p>
</li>
</ul>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/cluster/plot_adjusted_for_chance_measures.html"><img alt="../_images/plot_adjusted_for_chance_measures_0011.png" src="../_images/plot_adjusted_for_chance_measures_0011.png" style="width: 800.0px; height: 600.0px;" /></a>
</div>
<ul class="simple">
<li>These metrics <strong>require the knowledge of the ground truth classes</strong> while
almost never available in practice or requires manual assignment by
human annotators (as in the supervised learning setting).</li>
</ul>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_adjusted_for_chance_measures.html#example-cluster-plot-adjusted-for-chance-measures-py"><em>Adjustment for chance in clustering performance evaluation</em></a>: Analysis of
the impact of the dataset size on the value of clustering measures
for random assignments.</li>
</ul>
</div>
</div>
<div class="section" id="id16">
<h4>2.3.9.3.3. Mathematical formulation<a class="headerlink" href="#id16" title="Permalink to this headline">Â¶</a></h4>
<p>Homogeneity and completeness scores are formally given by:</p>
<div class="math">
<p><span class="math">h = 1 - \frac{H(C|K)}{H(C)}</span></p>
</div><div class="math">
<p><span class="math">c = 1 - \frac{H(K|C)}{H(K)}</span></p>
</div><p>where <span class="math">H(C|K)</span> is the <strong>conditional entropy of the classes given
the cluster assignments</strong> and is given by:</p>
<div class="math">
<p><span class="math">H(C|K) = - \sum_{c=1}^{|C|} \sum_{k=1}^{|K|} \frac{n_{c,k}}{n}
\cdot \log\left(\frac{n_{c,k}}{n_k}\right)</span></p>
</div><p>and <span class="math">H(C)</span> is the <strong>entropy of the classes</strong> and is given by:</p>
<div class="math">
<p><span class="math">H(C) = - \sum_{c=1}^{|C|} \frac{n_c}{n} \cdot \log\left(\frac{n_c}{n}\right)</span></p>
</div><p>with <span class="math">n</span> the total number of samples, <span class="math">n_c</span> and <span class="math">n_k</span>
the number of samples respectively belonging to class <span class="math">c</span> and
cluster <span class="math">k</span>, and finally <span class="math">n_{c,k}</span> the number of samples
from class <span class="math">c</span> assigned to cluster <span class="math">k</span>.</p>
<p>The <strong>conditional entropy of clusters given class</strong> <span class="math">H(K|C)</span> and the
<strong>entropy of clusters</strong> <span class="math">H(K)</span> are defined in a symmetric manner.</p>
<p>Rosenberg and Hirschberg further define <strong>V-measure</strong> as the <strong>harmonic
mean of homogeneity and completeness</strong>:</p>
<div class="math">
<p><span class="math">v = 2 \cdot \frac{h \cdot c}{h + c}</span></p>
</div><div class="topic">
<p class="topic-title first">References</p>
<table class="docutils citation" frame="void" id="rh2007" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[RH2007]</td><td><a class="reference external" href="http://aclweb.org/anthology/D/D07/D07-1043.pdf">V-Measure: A conditional entropy-based external cluster evaluation
measure</a>
Andrew Rosenberg and Julia Hirschberg, 2007</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="b2011" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id13">[B2011]</a></td><td><a class="reference external" href="http://www.cs.columbia.edu/~hila/hila-thesis-distributed.pdf">Identication and Characterization of Events in Social Media</a>, Hila
Becker, PhD Thesis.</td></tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="silhouette-coefficient">
<span id="id17"></span><h3>2.3.9.4. Silhouette Coefficient<a class="headerlink" href="#silhouette-coefficient" title="Permalink to this headline">Â¶</a></h3>
<p>If the ground truth labels are not known, evaluation must be performed using
the model itself. The Silhouette Coefficient
(<a class="reference internal" href="generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score" title="sklearn.metrics.silhouette_score"><tt class="xref py py-func docutils literal"><span class="pre">sklearn.metrics.silhouette_score</span></tt></a>)
is an example of such an evaluation, where a
higher Silhouette Coefficient score relates to a model with better defined
clusters. The Silhouette Coefficient is defined for each sample and is composed
of two scores:</p>
<ul class="simple">
<li><strong>a</strong>: The mean distance between a sample and all other points in the same
class.</li>
<li><strong>b</strong>: The mean distance between a sample and all other points in the <em>next
nearest cluster</em>.</li>
</ul>
<p>The Silhouette Coefficient <em>s</em> for a single sample is then given as:</p>
<div class="math">
<p><span class="math">s = \frac{b - a}{max(a, b)}</span></p>
</div><p>The Silhouette Coefficient for a set of samples is given as the mean of the
Silhouette Coefficient for each sample.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">pairwise_distances</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
<p>In normal usage, the Silhouette Coefficient is applied to the results of a
cluster analysis.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans_model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans_model</span><span class="o">.</span><span class="n">labels_</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">silhouette_score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s">&#39;euclidean&#39;</span><span class="p">)</span>
<span class="gp">... </span>                                                     
<span class="go">0.55...</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">References</p>
<ul class="simple">
<li>Peter J. Rousseeuw (1987). &#8220;Silhouettes: a Graphical Aid to the
Interpretation and Validation of Cluster Analysis&#8221;. Computational
and Applied Mathematics 20: 53â65.
<a class="reference external" href="http://dx.doi.org/10.1016/0377-0427(87)90125-7">doi:10.1016/0377-0427(87)90125-7</a>.</li>
</ul>
</div>
<div class="section" id="id18">
<h4>2.3.9.4.1. Advantages<a class="headerlink" href="#id18" title="Permalink to this headline">Â¶</a></h4>
<ul class="simple">
<li>The score is bounded between -1 for incorrect clustering and +1 for highly
dense clustering. Scores around zero indicate overlapping clusters.</li>
<li>The score is higher when clusters are dense and well separated, which relates
to a standard concept of a cluster.</li>
</ul>
</div>
<div class="section" id="id19">
<h4>2.3.9.4.2. Drawbacks<a class="headerlink" href="#id19" title="Permalink to this headline">Â¶</a></h4>
<ul class="simple">
<li>The Silhouette Coefficient is generally higher for convex clusters than other
concepts of clusters, such as density based clusters like those obtained
through DBSCAN.</li>
</ul>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_kmeans_silhouette_analysis.html#example-cluster-plot-kmeans-silhouette-analysis-py"><em>Selecting the number of clusters with silhouette analysis on KMeans clustering</em></a> : In this example
the silhouette analysis is used to choose an optimal value for n_clusters.</li>
</ul>
</div>
</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer">
        &copy; 2010 - 2014, scikit-learn developers (BSD License).
      <a href="../_sources/modules/clustering.txt" rel="nofollow">Show this page source</a>
    </div>
     <div class="rel">
    
    <div class="buttonPrevious">
      <a href="manifold.html">Previous
      </a>
    </div>
    
     </div>

    
    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-22606712-2']);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
    

    <script src="http://www.google.com/jsapi" type="text/javascript"></script>
    <script type="text/javascript"> google.load('search', '1',
        {language : 'en'}); google.setOnLoadCallback(function() {
            var customSearchControl = new
            google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');
            customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);
            var options = new google.search.DrawOptions();
            options.setAutoComplete(true);
            customSearchControl.draw('cse', options); }, true);
    </script>
  </body>
</html>